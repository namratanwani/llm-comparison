{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db39a858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import pandas as pd\n",
    "import ollama\n",
    "from openai import OpenAI\n",
    "from google import genai\n",
    "import anthropic\n",
    "\n",
    "gemini_key = os.getenv(\"GEMINI_KEY\")\n",
    "os.environ[\"GEMINI_API_KEY\"] = gemini_key\n",
    "claude_key = os.getenv(\"CLAUDE_KEY\")\n",
    "openai_key = os.getenv(\"OPENAI_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aacb2b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "TASK:\n",
    "Identify who is the patient and who is the attending medical professional(s) in the scenario below.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Read the scenario carefully.\n",
    "2. Identify the patient by their name and role.\n",
    "3. Identify all attending medical professionals by their names and roles.\n",
    "4. ONLY list all identified names in the specified format.\n",
    "5. Do not use numbers or bullet points.\n",
    "6. Names should be separated by commas.\n",
    "7. Do not include any additional information or context or explanations.\n",
    "8. Ensure the output is strictly formatted as specified below.\n",
    "9. Do not include extra characters for formatting.\n",
    "\n",
    "STRICT OUTPUT FORMAT:\n",
    "Patient: patient_name1, patient_name2, patient_name3\n",
    "Medical Professional(s): attending_medical_professional_name1, attending_medical_professional_name2, attending_medical_professional_name3\n",
    "\n",
    "SCENARIO:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ee337679",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Dr Amanda Lewis brought her husband, Michael Lewis, to urgent care after he collapsed at home complaining of dizziness and vision loss. Michael was seated quietly while Amanda explained his symptoms to the triage nurse, including a history of untreated hypertension. Nurse Tom Bradley took Michaelâ€™s vitals and noted a high blood pressure reading. Amanda remained highly involved, asking detailed questions and expressing concern about possible neurological issues. Michael, who seemed disoriented, responded briefly when asked. Dr. Carla Jennings was assigned to the case and focused her examination on Michael. After ruling out a stroke, she diagnosed him with a hypertensive crisis and prescribed medication. Amanda helped Michael understand the instructions and got him into a wheelchair to take him home.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67db97ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_claude_response(prompt, context):\n",
    "    client = anthropic.Anthropic(api_key=claude_key)\n",
    "\n",
    "    message = client.messages.create(\n",
    "        model=\"claude-opus-4-20250514\",\n",
    "        max_tokens=1000,\n",
    "        temperature=1,\n",
    "        system=prompt,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"\\n\\n\" + context\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    # print(message.content[0].text)\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c938479c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gemini_response(prompt, context):\n",
    "    client = genai.Client()\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=prompt + \"\\n\\n\" + context\n",
    "    )\n",
    "    # print(response.text)\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5103993",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_gpt_response(prompt, context):\n",
    "    client = OpenAI(\n",
    "    api_key=openai_key  \n",
    ")\n",
    "    response = client.chat.completions.create(\n",
    "                    \n",
    "                    messages=[\n",
    "                    {\"role\": \"system\", \"content\": prompt},\n",
    "                    {\"role\": \"user\", \"content\": context},\n",
    "                    ],\n",
    "                    temperature=0,\n",
    "                    model=\"gpt-4.1\", \n",
    "                    seed=48,\n",
    "                    max_tokens=5000\n",
    "                )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b34e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ollama_response(prompt, context, model=\"llama3.1:8b\"):\n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{prompt}\\n\\n{context}\"\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    return response['message']['content']\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "87d3328a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n",
      "Iteration 10\n",
      "Iteration 11\n",
      "Iteration 12\n",
      "Iteration 13\n",
      "Iteration 14\n",
      "Iteration 15\n",
      "Iteration 16\n",
      "Iteration 17\n",
      "Iteration 18\n",
      "Iteration 19\n",
      "Iteration 20\n",
      "Iteration 21\n",
      "Iteration 22\n",
      "Iteration 23\n",
      "Iteration 24\n",
      "Iteration 25\n",
      "Iteration 26\n",
      "Iteration 27\n",
      "Iteration 28\n",
      "Iteration 29\n",
      "Iteration 30\n",
      "Iteration 31\n",
      "Iteration 32\n",
      "Iteration 33\n",
      "Iteration 34\n",
      "Iteration 35\n",
      "Iteration 36\n",
      "Iteration 37\n",
      "Iteration 38\n",
      "Iteration 39\n",
      "Iteration 40\n",
      "Iteration 41\n",
      "Iteration 42\n",
      "Iteration 43\n",
      "Iteration 44\n",
      "Iteration 45\n",
      "Iteration 46\n",
      "Iteration 47\n",
      "Iteration 48\n",
      "Iteration 49\n",
      "Iteration 50\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "all_models = []\n",
    "\n",
    "for i in range(50):\n",
    "    print(f\"Iteration {i+1}\")\n",
    "    model = \"gemma3:12b\"\n",
    "    result = get_ollama_response(prompt, context, model=model)\n",
    "    all_results.append(result)\n",
    "    all_models.append(model)\n",
    "\n",
    "    model = \"llama3.1:8b\"\n",
    "    result = get_ollama_response(prompt, context, model=model)\n",
    "    all_results.append(result)\n",
    "    all_models.append(model)\n",
    "\n",
    "    model = \"gemini2.5-flash\"\n",
    "    result = get_gemini_response(prompt, context)\n",
    "    all_results.append(result)\n",
    "    all_models.append(model)\n",
    "\n",
    "    model = \"claude-opus-4-20250514\"\n",
    "    result = get_claude_response(prompt, context)\n",
    "    all_results.append(result)\n",
    "    all_models.append(model)\n",
    "\n",
    "    model = \"gpt-4.1\"\n",
    "    result = get_gpt_response(prompt, context)\n",
    "    all_results.append(result)\n",
    "    all_models.append(model)\n",
    "\n",
    "    d = {\n",
    "        \"model\": all_models,\n",
    "        \"result\": all_results\n",
    "    }\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(d)\n",
    "    df.to_csv(\"llm_comparison_results.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219802c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
